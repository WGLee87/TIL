
1. ë¶„ë¥˜ëª¨í˜•ì˜ ì¢…ë¥˜, ë¶„ë¥˜ ì„±ëŠ¥í‰ê°€

- í¬ê²Œ 2ê°€ì§€ë¡œ ë‚˜ë‰˜ëŠ”ë°,
	- ì²«ì§¸ í™•ë¥ ì  ëª¨í˜•
	- ë‘˜ì§¸ íŒë³„í•¨ìˆ˜ ëª¨í˜•
	ì´ë‹¤.


```python
import matplotlib
from matplotlib import font_manager, rc
import platform

try :
    if platform.system() == 'windows':
        # windowsì˜ ê²½ìš°
        font_name = font_manager.FomntProperties(fname="c:/Windows/Font")
        rc('font', family = font_name)
    else:
        # macì˜ ê²½ìš°
        rc('font', family = 'AppleGothic')
except :
    pass

matplotlib.rcParams['axes.unicode_minus'] = False
```

###### ë¶„ë¥˜ëª¨í˜•ì˜ ì¢…ë¥˜

    1. í™•ë¥ ì  ëª¨í˜•(ë°©ë²•ë¡ )
        (1) ì§ì ‘ ì¡°ê±´ë¶€í™•ë¥  í•¨ìˆ˜ì˜ ëª¨ì–‘ì„ ì¶”ì •í•˜ëŠ” í™•ë¥ ì  íŒë³„ëª¨í˜•
        (2) *ë² ì´ì¦ˆ ì •ë¦¬ë¥¼ ì‚¬ìš©í•œ* ê°„ì ‘ì ìœ¼ë¡œ ì¶”ì •í•˜ëŠ” í™•ë¥ ì  ìƒì„±ëª¨í˜•
    2. íŒë³„í•¨ìˆ˜ ëª¨í˜•(ë°©ë²•ë¡ )
    
    * ëª¨í˜•	       *  ë°©ë²•ë¡ 
    LDA/QDA	í™•ë¥ ì  ìƒì„±ëª¨í˜•
    ë‚˜ì´ë¸Œ ë² ì´ì§€ì•ˆ	í™•ë¥ ì  ìƒì„±ëª¨í˜•
    ë¡œì§€ìŠ¤í‹± íšŒê·€	í™•ë¥ ì  íŒë³„ëª¨í˜•
    ì˜ì‚¬ê²°ì •ë‚˜ë¬´	í™•ë¥ ì  íŒë³„ëª¨í˜•
    í¼ì…‰íŠ¸ë¡ 	íŒë³„í•¨ìˆ˜ ëª¨í˜•
    ì„œí¬íŠ¸ë²¡í„°ë¨¸ì‹ 	íŒë³„í•¨ìˆ˜ ëª¨í˜•
    ì¸ê³µì‹ ê²½ë§	íŒë³„í•¨ìˆ˜ ëª¨í˜•
  

### í™•ë¥ ì  ëª¨í˜•

    ì‚¬ì´í‚·ëŸ° íŒ¨í‚¤ì§€ì—ì„œ ì¡°ê±´ë¶€í™•ë¥   ğ‘ƒ(ğ‘¦=ğ‘˜|ğ‘¥) ì„ ì‚¬ìš©í•˜ëŠ” ë¶„ë¥˜ëª¨í˜•ë“¤ì€ ëª¨ë‘ predict_proba ë©”ì„œë“œì™€ predict_log_proba ë©”ì„œë“œë¥¼ ì§€ì›í•œë‹¤. ì´ ë©”ì„œë“œë“¤ì€ ë…ë¦½ë³€ìˆ˜  ğ‘¥ ê°€ ì£¼ì–´ì§€ë©´ ì¢…ì†ë³€ìˆ˜  ğ‘¦ ì˜ ëª¨ë“  ì¹´í…Œê³ ë¦¬ ê°’ì— ëŒ€í•´ ì¡°ê±´ë¶€í™•ë¥  ë˜ëŠ” ì¡°ê±´ë¶€í™•ë¥ ì˜ ë¡œê·¸ê°’ì„ ê³„ì‚°í•œë‹¤.
    
    log í™•ë¥ ê°’ì„ ì“°ëŠ” ì´ìœ ëŠ” ê°’ì´ ì•„ì£¼ ì‘ì€ ê²½ìš° ë¶€ë™ì†Œìˆ˜ì ì˜ êµ¬ë³„ì´ ì–´ë µê³  ì˜¤ì°¨ê°€ ë°œìƒí•  ìš°ë ¤ê°€ ìˆì–´ì„œ ê·¸ê²ƒì„ ë” ì •í™•í•˜ê²Œ ë‚˜íƒ€ë‚´ê¸° ìœ„í•´ ì‚¬ìš©

##### í™•ë¥ ì  ìƒì„± ëª¨í˜•

###### QDA
    QDA(Quadratic Discriminant Analysis)ëŠ” ì¡°ê±´ë¶€í™•ë¥  ê¸°ë°˜ ìƒì„±(generative) ëª¨í˜•ì˜ í•˜ë‚˜ì´ë‹¤.


```python
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
y = np.array([0, 0, 0, 1, 1, 1])

plt.scatter(X[:3, 0], X[:3, 1], c="k", s=100, edgecolor='k', linewidth=2, label="y=0")
plt.scatter(X[3:, 0], X[3:, 1], c="w", s=100, edgecolor='k', linewidth=2, label="y=1")
plt.title("í•™ìŠµìš© ë°ì´í„°")
plt.xlabel("x1")
plt.ylabel("x2")
plt.legend()
plt.show()
```


<img width="399" alt="output_5_0" src="https://user-images.githubusercontent.com/59719711/82555383-9da40200-9ba2-11ea-96a3-5a0374106dee.png">



```python
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

model = QuadraticDiscriminantAnalysis().fit(X,y)
model
```




    QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                                  store_covariance=False, tol=0.0001)




```python
x = [[0, 0]]
p = model.predict_proba(x)[0]

plt.subplot(211)
plt.scatter(X[:3, 0], X[:3, 1], c="k", s=100, edgecolor='k', linewidth=2, label="y=0")
plt.scatter(X[3:, 0], X[3:, 1], c="w", s=100, edgecolor='k', linewidth=2, label="y=1")
plt.scatter(x[0][0], x[0][1], c='r', s=100, edgecolor='k', marker='x', linewidth=5)
plt.title("í…ŒìŠ¤íŠ¸ ë°ì´í„°")
plt.xlabel("x1")
plt.ylabel("x2")
plt.subplot(212)
plt.bar(model.classes_, p)
plt.title("ì¡°ê±´ë¶€ í™•ë¥ ë¶„í¬")
plt.gca().xaxis.grid(False)
plt.xticks(model.classes_, ["$P(y=0|x_{test})$", "$P(y=1|x_{test})$"])
plt.ylabel("í™•ë¥ ê°’")
plt.tight_layout()
plt.show()
```


<img width="420" alt="output_7_0" src="https://user-images.githubusercontent.com/59719711/82555399-a7c60080-9ba2-11ea-9270-425eac2f118a.png">


###### ë‚˜ì´ë¸Œ ë² ì´ì§€ì•ˆ ëª¨í˜•
    ì¡°ê±´ë¶€í™•ë¥  ê¸°ë°˜ ìƒì„± ëª¨í˜•ì˜ ì¥ì  ì¤‘ í•˜ë‚˜ëŠ” í´ë˜ìŠ¤ê°€ 3ê°œ ì´ìƒì¸ ê²½ìš°ì—ë„ ë°”ë¡œ ì ìš©í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì´ë©°, ë‚˜ì´ë¸Œ ë² ì´ì§€ì•ˆ(Naive Bayesian) ëª¨í˜•ë„ ì¡°ê±´ë¶€í™•ë¥  ëª¨í˜•ì˜ ì¼ì¢…ì´ë‹¤.


```python
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline

news = fetch_20newsgroups(subset="all")
model = Pipeline([
    ('vect', TfidfVectorizer(stop_words="english")),
    ('nb', MultinomialNB()),
])
model.fit(news.data, news.target)

n = 1
x = news.data[n:n + 1]
y = model.predict(x)[0]
print(x[0])
print("=" * 80)
print("ì‹¤ì œ í´ë˜ìŠ¤:", news.target_names[news.target[n]])
print("ì˜ˆì¸¡ í´ë˜ìŠ¤:", news.target_names[y])
```

    From: mblawson@midway.ecn.uoknor.edu (Matthew B Lawson)
    Subject: Which high-performance VLB video card?
    Summary: Seek recommendations for VLB video card
    Nntp-Posting-Host: midway.ecn.uoknor.edu
    Organization: Engineering Computer Network, University of Oklahoma, Norman, OK, USA
    Keywords: orchid, stealth, vlb
    Lines: 21
    
      My brother is in the market for a high-performance video card that supports
    VESA local bus with 1-2MB RAM.  Does anyone have suggestions/ideas on:
    
      - Diamond Stealth Pro Local Bus
    
      - Orchid Farenheit 1280
    
      - ATI Graphics Ultra Pro
    
      - Any other high-performance VLB card
    
    
    Please post or email.  Thank you!
    
      - Matt
    
    -- 
        |  Matthew B. Lawson <------------> (mblawson@essex.ecn.uoknor.edu)  |   
      --+-- "Now I, Nebuchadnezzar, praise and exalt and glorify the King  --+-- 
        |   of heaven, because everything he does is right and all his ways  |   
        |   are just." - Nebuchadnezzar, king of Babylon, 562 B.C.           |   
    
    ================================================================================
    ì‹¤ì œ í´ë˜ìŠ¤: comp.sys.ibm.pc.hardware
    ì˜ˆì¸¡ í´ë˜ìŠ¤: comp.sys.ibm.pc.hardware



```python
plt.subplot(211)
plt.bar(model.classes_, model.predict_proba(x)[0])
plt.xlim(-1, 20)
plt.gca().xaxis.grid(False)
plt.xticks(model.classes_)
plt.xlabel("ì¹´í…Œê³ ë¦¬(í´ë˜ìŠ¤)")
plt.ylabel("í™•ë¥ ê°’")

plt.subplot(212)
plt.bar(model.classes_, model.predict_log_proba(x)[0])
plt.xlim(-1, 20)
plt.gca().xaxis.grid(False)
plt.xticks(model.classes_)
plt.xlabel("ì¹´í…Œê³ ë¦¬(í´ë˜ìŠ¤)")
plt.ylabel("í™•ë¥ ì˜ ë¡œê·¸ê°’")
plt.suptitle("ì¡°ê±´ë¶€ í™•ë¥ ë¶„í¬")

plt.show()
```


<img width="393" alt="output_10_0" src="https://user-images.githubusercontent.com/59719711/82555427-b57b8600-9ba2-11ea-8fee-5837cc30f419.png">



##### í™•ë¥ ì  íŒë³„ ëª¨í˜•
    í™•ë¥ ì  ìƒì„± ëª¨í˜•ì˜ ê²½ìš° ë² ì´ì¦ˆ ì •ë¦¬ë¥¼ í†µí•´ êµ¬í•œ ë°˜ë©´ íŒë³„ ëª¨í˜•ì€ ì§ì ‘ ì°¾ì•„ë‚´ëŠ” ë°©ë²•
    
        p(y = k | x = f(x)
        
    ë‹¨, ì´ í•¨ìˆ˜  ğ‘“(ğ‘¥) ëŠ” 0ë³´ë‹¤ ê°™ê±°ë‚˜ í¬ê³  1ë³´ë‹¤ ê°™ê±°ë‚˜ ì‘ë‹¤ëŠ” ì¡°ê±´ì„ ë§Œì¡±í•´ì•¼ í•œë‹¤.

###### ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨í˜•
    ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨í˜•ì€ í™•ë¥ ë¡ ì  íŒë³„ ëª¨í˜•ì— ì†í•œë‹¤.


```python
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression

X0, y = make_classification(n_features=1, n_redundant=0,
                            n_informative=1, n_clusters_per_class=1, random_state=4)

model = LogisticRegression().fit(X0, y)

xx = np.linspace(-3, 3, 100)
XX = xx[:, np.newaxis]

prob = model.predict_proba(XX)[:, 1]
# prob = 1.0/(1 + np.exp(-model.coef_[0][0]*xx - model.intercept_[0]))

x_test = [[-0.2]]

plt.subplot(211)
plt.plot(xx, prob)
plt.scatter(X0, y, marker='o', c=y, s=100, edgecolor='k', linewidth=2)
plt.scatter(x_test[0], model.predict_proba(x_test)[0][1:], marker='x', s=500, c='r', lw=5)
plt.xlim(-3, 3)
plt.ylim(-.2, 1.2)
plt.xlabel("x")
plt.ylabel("y=0 ë˜ëŠ” y=1ì¼ í™•ë¥ ")
plt.legend(["$P(y=1|x_{test})$"])
plt.subplot(212)
plt.bar(model.classes_, model.predict_proba(x_test)[0])
plt.xlim(-1, 2)
plt.gca().xaxis.grid(False)
plt.xticks(model.classes_, ["$P(y=0|x_{test})$", "$P(y=1|x_{test})$"])
plt.title("ì¡°ê±´ë¶€ í™•ë¥ ë¶„í¬")
plt.xlabel("ì¹´í…Œê³ ë¦¬(í´ë˜ìŠ¤)")
plt.ylabel("í™•ë¥ ê°’")
plt.tight_layout()
plt.show()
```


<img width="427" alt="output_13_0" src="https://user-images.githubusercontent.com/59719711/82555457-c1ffde80-9ba2-11ea-92e0-071969022a1d.png">




```python

```

### íŒë³„í•¨ìˆ˜ ëª¨í˜•
    ë˜ ë‹¤ë¥¸ ë¶„ë¥˜ ë°©ë²•ì€ ë™ì¼í•œ í´ë˜ìŠ¤ê°€ ëª¨ì—¬ ìˆëŠ” ì˜ì—­ê³¼ ê·¸ ì˜ì—­ì„ ë‚˜ëˆ„ëŠ” ê²½ê³„ë©´(boundary plane)ì„ ì •ì˜í•˜ëŠ” ê²ƒ.
    
            íŒë³„ ê²½ê³„ì„ :ğ‘“(ğ‘¥)=0
            í´ë˜ìŠ¤ 1:ğ‘“(ğ‘¥)>0
            í´ë˜ìŠ¤ 0:ğ‘“(ğ‘¥)<0

    ì‚¬ì´í‚·ëŸ°ì—ì„œ íŒë³„í•¨ìˆ˜ ëª¨í˜•ì€ íŒë³„í•¨ìˆ˜ê°’ì„ ì¶œë ¥í•˜ëŠ” decision_function ë©”ì„œë“œë¥¼ ì œê³µí•œë‹¤.
        * model.decision_function(x) > ë‚˜ì˜¤ëŠ” ê°’ì— ë”°ë¼ í´ë˜ìŠ¤ íŒë³„

###### í¼ì…‰íŠ¸ë¡ 
    í¼ì…‰íŠ¸ë¡ (Perceptron)ì€ ê°€ì¥ ë‹¨ìˆœí•œ íŒë³„í•¨ìˆ˜ ëª¨í˜•ì´ë‹¤. ë‹¤ìŒ ê·¸ë¦¼ê³¼ ê°™ì´ ì§ì„ ì´ ê²½ê³„ì„ (boundary line)ìœ¼ë¡œ ë°ì´í„° ì˜ì—­ì„ ë‚˜ëˆˆë‹¤.


```python
from sklearn.linear_model import Perceptron
from sklearn.datasets import load_iris
iris = load_iris()
idx = np.in1d(iris.target, [0, 2])
X = iris.data[idx, 0:2]
y = iris.target[idx]

model = Perceptron(max_iter=100, eta0=0.1, random_state=1).fit(X, y)
XX_min, XX_max = X[:, 0].min() - 1, X[:, 0].max() + 1
YY_min, YY_max = X[:, 1].min() - 1, X[:, 1].max() + 1
XX, YY = np.meshgrid(np.linspace(XX_min, XX_max, 1000),
                     np.linspace(YY_min, YY_max, 1000))
ZZ = model.predict(np.c_[XX.ravel(), YY.ravel()]).reshape(XX.shape)
plt.contour(XX, YY, ZZ, colors='k')
plt.scatter(X[:, 0], X[:, 1], c=y, s=30, edgecolor='k', linewidth=1)

idx = [22, 36, 70, 80]
plt.scatter(X[idx, 0], X[idx, 1], c='r', s=100, alpha=0.5)
for i in idx:
    plt.annotate(i, xy=(X[i, 0], X[i, 1] + 0.1))
plt.grid(False)
plt.title("í¼ì…‰íŠ¸ë¡ ì˜ íŒë³„ì˜ì—­")
plt.xlabel("x1")
plt.ylabel("x2")
plt.show()
```


<img width="393" alt="output_17_0" src="https://user-images.githubusercontent.com/59719711/82555480-cf1ccd80-9ba2-11ea-8cee-6c5ff498af44.png">



```python
plt.bar(range(len(idx)), model.decision_function(X[idx]))
plt.xticks(range(len(idx)), idx)
plt.gca().xaxis.grid(False)
plt.title("ê° ë°ì´í„°ì˜ íŒë³„í•¨ìˆ˜ ê°’")
plt.xlabel("í‘œë³¸ ë²ˆí˜¸")
plt.ylabel("íŒë³„í•¨ìˆ˜ê°’ f(x)")
plt.show()
```


<img width="387" alt="output_18_0" src="https://user-images.githubusercontent.com/59719711/82555500-dc39bc80-9ba2-11ea-8ecd-cb54055bbec2.png">



```python
from mpl_toolkits.mplot3d import Axes3D
from sklearn.datasets import load_iris

iris = load_iris()
X = iris.data[:, :2]
y = iris.target
idx = np.logical_or(iris.target == 0, iris.target == 1)
X = iris.data[idx, :3]
y = iris.target[idx]

fig = plt.figure()
ax = Axes3D(fig, elev=20, azim=10)
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, marker='o', s=100, cmap=mpl.cm.jet)
ax.plot_surface(np.array([[4, 4], [7, 7]]), np.array([[2, 4.5], [2, 4.5]]),
                np.array([[2, 4], [2, 4]]), color='g', alpha=.3)
plt.title("3ì°¨ì› íŠ¹ì§•ë°ì´í„°ì˜ íŒë³„ê²½ê³„")
plt.xlabel("x")
plt.ylabel("y")
ax.set_zlabel("z")
plt.show()
# í•˜ì´í¼ í”Œë ˆì¸(decision hyperplane)
```


<img width="446" alt="output_19_0" src="https://user-images.githubusercontent.com/59719711/82555527-e6f45180-9ba2-11ea-9144-c572400b3d8e.png">


###### ì»¤ë„ SVM(Kernel Support Vector Machine)


```python
from sklearn import svm

xx, yy = np.meshgrid(np.linspace(-3, 3, 500),
                     np.linspace(-3, 3, 500))
np.random.seed(0)
X = np.random.randn(300, 2)
Y = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)

model = svm.NuSVC().fit(X, Y)
Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.imshow(Z, interpolation='nearest',
           extent=(xx.min(), xx.max(), yy.min(), yy.max()), aspect='auto',
           origin='lower', cmap=plt.cm.PuOr_r)
contours = plt.contour(xx, yy, Z, levels=[0], linewidths=3)
plt.scatter(X[:, 0], X[:, 1], s=30, c=Y, cmap=plt.cm.Paired)
idx = [0, 20, 40, 60]
plt.scatter(X[idx, 0], X[idx, 1], c=Y[idx], s=200, alpha=0.5)
for i in idx:
    plt.annotate(i, xy=(X[i, 0], X[i, 1]+0.15), color='white')
plt.grid(False)
plt.axis([-3, 3, -3, 3])
plt.xlabel("x1")
plt.ylabel("x2")
plt.title("ì»¤ë„ SVMì˜ íŒë³„ì˜ì—­")
plt.show()
```


<img width="391" alt="output_21_0" src="https://user-images.githubusercontent.com/59719711/82555553-f378aa00-9ba2-11ea-83e7-cf75713eaa74.png">



```python
plt.bar(range(len(idx)), model.decision_function(X[idx]))
plt.xticks(range(len(idx)), idx)
plt.gca().xaxis.grid(False)
plt.xlabel("í‘œë³¸ ë²ˆí˜¸")
plt.ylabel("íŒë³„í•¨ìˆ˜ê°’ f(x)")
plt.title("ê° ë°ì´í„°ì˜ íŒë³„í•¨ìˆ˜ ê°’")
plt.show()
```


<img width="387" alt="output_22_0" src="https://user-images.githubusercontent.com/59719711/82555597-ff646c00-9ba2-11ea-9719-e4569ce5f62a.png">


### ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜
    í™•ë¥ ì  ëª¨í˜•ì€ í´ë˜ìŠ¤ê°€ 3ê°œ ì´ìƒì¸ ê²½ìš°ë¥¼ ë‹¤ì¤‘ í´ë˜ìŠ¤(Multi-Class) ë¶„ë¥˜ë¬¸ì œë„ í’€ ìˆ˜ ìˆì§€ë§Œ íŒë³„í•¨ìˆ˜ ëª¨í˜•ì€ ì¢…ì†ë³€ìˆ˜ì˜ í´ë˜ìŠ¤ê°€ 2ê°œì¸ ê²½ìš°ë¥¼ ì´ì§„(Binary Class) ë¶„ë¥˜ë¬¸ì œë°–ì—ëŠ” í’€ì§€ ëª»í•œë‹¤.

    ì´ë•ŒëŠ” OvO (One-Vs-One) ë°©ë²•ì´ë‚˜ OvR (One-vs-the-Rest) ë°©ë²• ë“±ì„ ì´ìš©í•˜ì—¬ ì—¬ëŸ¬ê°œì˜ ì´ì§„ í´ë˜ìŠ¤ ë¶„ë¥˜ë¬¸ì œë¡œ ë³€í™˜í•˜ì—¬ í‘¼ë‹¤.

##### OvO ë°©ë²•
    OvO (One-Vs-One) ë°©ë²•ì€  ğ¾ ê°œì˜ í´ë˜ìŠ¤ê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš°, ì´ ì¤‘ 2ê°œì˜ í´ë˜ìŠ¤ ì¡°í•©ì„ ì„ íƒí•˜ì—¬  ğ¾(ğ¾âˆ’1)/2 ê°œì˜ ì´ì§„ í´ë˜ìŠ¤ ë¶„ë¥˜ë¬¸ì œë¥¼ í’€ê³  ì´ì§„ ë¶„ë¥˜ë¬¸ì œë¥¼ í’€ì–´ ê°€ì¥ ë§ì€ ê²°ê³¼ê°€ ë‚˜ì˜¨ í´ë˜ìŠ¤ë¥¼ ì„ íƒí•˜ëŠ” ë°©ë²•ì´ë‹¤. ì„ íƒë°›ì€ íšŸìˆ˜ë¡œ ì„ íƒí•˜ë©´ íšŸìˆ˜ê°€ ê°™ì€ ê²½ìš°ë„ ë‚˜ì˜¬ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ê° í´ë˜ìŠ¤ê°€ ì–»ì€ ì¡°ê±´ë¶€ í™•ë¥ ê°’ì„ ëª¨ë‘ ë”í•œ ê°’ì„ ë¹„êµí•˜ì—¬ ê°€ì¥ í° ì¡°ê±´ë¶€ í™•ë¥  ì´í•©ì„ ê°€ì§„ í´ë˜ìŠ¤ë¥¼ ì„ íƒí•œë‹¤.

##### OvR ë°©ë²•
    OvO ë°©ë²•ì€ í´ë˜ìŠ¤ì˜ ìˆ˜ê°€ ë§ì•„ì§€ë©´ ì‹¤í–‰í•´ì•¼ í•  ì´ì§„ ë¶„ë¥˜ë¬¸ì œì˜ ìˆ˜ê°€ ë„ˆë¬´ ë§ì•„ì§„ë‹¤.
    OvR(One-vs-the-Rest) ë°©ë²•ì€  ğ¾ ê°œì˜ í´ë˜ìŠ¤ê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš°, ê°ê°ì˜ í´ë˜ìŠ¤ì— ëŒ€í•´ í‘œë³¸ì´ ì†í•˜ëŠ”ì§€(y=1) ì†í•˜ì§€ ì•ŠëŠ”ì§€(y=0)ì˜ ì´ì§„ ë¶„ë¥˜ë¬¸ì œë¥¼ í‘¼ë‹¤. OvOì™€ ë‹¬ë¦¬ í´ë˜ìŠ¤ ìˆ˜ë§Œí¼ì˜ ì´ì§„ ë¶„ë¥˜ë¬¸ì œë¥¼ í’€ë©´ ëœë‹¤.
    OvRì—ì„œë„ íŒë³„ ê²°ê³¼ì˜ ìˆ˜ê°€ ê°™ì€ ë™ì  ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ê°€ ìˆê¸° ë•Œë¬¸ì— ê° í´ë˜ìŠ¤ê°€ ì–»ì€ ì¡°ê±´ë¶€ í™•ë¥ ê°’ì„ ë”í•´ì„œ ì´ ê°’ì´ ê°€ì¥ í° í´ë˜ìŠ¤ë¥¼ ì„ íƒí•œë‹¤.


```python

```


```python

```

# ë¶„ë¥˜ ì„±ëŠ¥í‰ê°€

    confusion_matrix(y_true, y_pred)
    accuracy_score(y_true, y_pred)
    precision_score(y_true, y_pred)
    recall_score(y_true, y_pred)
    fbeta_score(y_true, y_pred, beta)
    f1_score(y_true, y_pred)
    classfication_report(y_true, y_pred)
    roc_curve
    auc


```python
from sklearn.metrics import confusion_matrix

y_true = [2, 0, 2, 2, 0, 1]
y_pred = [0, 0, 2, 2, 0, 2]
```


```python
confusion_matrix(y_true, y_pred)
```




    array([[2, 0, 0],
           [0, 0, 1],
           [1, 0, 2]])



     ì‹¤ì œ/ì˜ˆì¸¡    P     N
       P(ì–‘ì„±)    TP    FN
       N(ìŒì„±)    FP    TN


```python
y_true = [1, 0, 1, 1, 0, 1]
y_pred = [0, 0, 1, 1, 0, 1]
confusion_matrix(y_true, y_pred)
```




    array([[2, 0],
           [1, 3]])




```python
# ìˆœì„œê°€ ì–‘ì„±, ìŒì„± ìˆœìœ¼ë¡œ ê³ ì • ì‹œí‚¤ë ¤ë©´
confusion_matrix(y_true, y_pred, labels=[1,0])
```




    array([[3, 1],
           [0, 2]])



###### í‰ê°€ì ìˆ˜
ì´ì§„ ë¶„ë¥˜í‰ê°€í‘œë¡œë¶€í„° í•˜ë‚˜ì˜ í‰ê°€ì ìˆ˜(score)ë¥¼ ê³„ì‚°í•˜ì—¬ ê·¸ ê°’ì„ ìµœì¢…ì ì¸ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤. ì´ ë•Œë„ ê´€ì ì— ë”°ë¼ ë‹¤ì–‘í•œ í‰ê°€ì ìˆ˜ê°€ ì“°ì¸ë‹¤.
    
    ì •í™•ë„Â¶
    ì •í™•ë„(accuracy)ëŠ” ì „ì²´ ìƒ˜í”Œ ì¤‘ ë§ê²Œ ì˜ˆì¸¡í•œ ìƒ˜í”Œ ìˆ˜ì˜ ë¹„ìœ¨ì„ ëœ»í•œë‹¤. ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ëª¨í˜•ì´ë‹¤. ì¼ë°˜ì ìœ¼ë¡œ í•™ìŠµì—ì„œ ìµœì í™” ëª©ì í•¨ìˆ˜ë¡œ ì‚¬ìš©ëœë‹¤.

    accuracy=ğ‘‡ğ‘ƒ+ğ‘‡ğ‘ / ğ‘‡ğ‘ƒ+ğ‘‡ğ‘+ğ¹ğ‘ƒ+ğ¹ğ‘

    ì •ë°€ë„Â¶
    ì •ë°€ë„(precision)ì€ ì–‘ì„± í´ë˜ìŠ¤ì— ì†í•œë‹¤ê³  ì¶œë ¥í•œ ìƒ˜í”Œ ì¤‘ ì‹¤ì œë¡œ ì–‘ì„± í´ë˜ìŠ¤ì— ì†í•˜ëŠ” ìƒ˜í”Œ ìˆ˜ì˜ ë¹„ìœ¨ì„ ë§í•œë‹¤. ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ëª¨í˜•ì´ë‹¤. FDSì˜ ê²½ìš°, ì‚¬ê¸° ê±°ë˜ë¼ê³  íŒë‹¨í•œ ê±°ë˜ ì¤‘ ì‹¤ì œ ì‚¬ê¸° ê±°ë˜ì˜ ë¹„ìœ¨ì´ ëœë‹¤.

    precision=ğ‘‡ğ‘ƒ / ğ‘‡ğ‘ƒ+ğ¹ğ‘ƒ

    ì¬í˜„ìœ¨Â¶
    ì¬í˜„ìœ¨(recall)ì€ ì‹¤ì œ ì–‘ì„± í´ë˜ìŠ¤ì— ì†í•œ í‘œë³¸ ì¤‘ì— ì–‘ì„± í´ë˜ìŠ¤ì— ì†í•œë‹¤ê³  ì¶œë ¥í•œ í‘œë³¸ì˜ ìˆ˜ì˜ ë¹„ìœ¨ì„ ëœ»í•œë‹¤. ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ëª¨í˜•ì´ë‹¤. FDSì˜ ê²½ìš° ì‹¤ì œ ì‚¬ê¸° ê±°ë˜ ì¤‘ì—ì„œ ì‹¤ì œ ì‚¬ê¸° ê±°ë˜ë¼ê³  ì˜ˆì¸¡í•œ ê±°ë˜ì˜ ë¹„ìœ¨ì´ ëœë‹¤. TPR(true positive rate) ë˜ëŠ” ë¯¼ê°ë„(sensitivity)ë¼ê³ ë„ í•œë‹¤.

    recall=ğ‘‡ğ‘ƒ / ğ‘‡ğ‘ƒ+ğ¹ğ‘

    ìœ„ì–‘ì„±ìœ¨Â¶
    ìœ„ì–‘ì„±ìœ¨(fall-out)ì€ ì‹¤ì œ ì–‘ì„± í´ë˜ìŠ¤ì— ì†í•˜ì§€ ì•ŠëŠ” í‘œë³¸ ì¤‘ì— ì–‘ì„± í´ë˜ìŠ¤ì— ì†í•œë‹¤ê³  ì¶œë ¥í•œ í‘œë³¸ì˜ ë¹„ìœ¨ì„ ë§í•œë‹¤. ë‹¤ë¥¸ í‰ê°€ì ìˆ˜ì™€ ë‹¬ë¦¬ ë‚®ì„ìˆ˜ë¡ ì¢‹ì€ ëª¨í˜•ì´ë‹¤. FDSì˜ ê²½ìš°ì—ëŠ” ì‹¤ì œë¡œëŠ” ì •ìƒ ê±°ë˜ì¸ë° FDSê°€ ì‚¬ê¸° ê±°ë˜ë¼ê³  ì˜ˆì¸¡í•œ ê±°ë˜ì˜ ë¹„ìœ¨ì´ ëœë‹¤. FPR(false positive rate)ë˜ëŠ” 1ì—ì„œ ìœ„ì–‘ì„±ë¥ ì˜ ê°’ì„ ëº€ ê°’ì„ íŠ¹ì´ë„(specificity)ë¼ê³ ë„ í•œë‹¤.

    fallout=ğ¹ğ‘ƒ / ğ¹ğ‘ƒ+ğ‘‡ğ‘

    Fì ìˆ˜Â¶
    ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ ê°€ì¤‘ì¡°í™”í‰ê· (weight harmonic average)ì„ Fì ìˆ˜(F-score)ë¼ê³  í•œë‹¤. ì •ë°€ë„ì— ì£¼ì–´ì§€ëŠ” ê°€ì¤‘ì¹˜ë¥¼ ë² íƒ€(beta)ë¼ê³  í•œë‹¤.

    ğ¹ğ›½=(1+ğ›½2)(precisionÃ—recall) / (ğ›½2precision+recall)

    ë² íƒ€ê°€ 1ì¸ ê²½ìš°ë¥¼ íŠ¹ë³„íˆ F1ì ìˆ˜ë¼ê³  í•œë‹¤.

    ğ¹1=2â‹…precisionâ‹…recall/(precision+recall)


```python
from sklearn.metrics import classification_report

y_true = [0, 0, 0, 1, 1, 0, 0]
y_pred = [0, 0, 0, 0, 1, 1, 1]

print(classification_report(y_true, y_pred, target_names=['class 0', 'class 1']))
```

                  precision    recall  f1-score   support
    
         class 0       0.75      0.60      0.67         5
         class 1       0.33      0.50      0.40         2
    
        accuracy                           0.57         7
       macro avg       0.54      0.55      0.53         7
    weighted avg       0.63      0.57      0.59         7
    



```python
y_true = [0, 0, 1, 1, 2, 2, 2]
y_pred = [0, 0, 1, 2, 2, 2, 1]
target_names = ['class 0', 'class 1', 'class 2']
print(classification_report(y_true, y_pred, target_names=target_names))
```

                  precision    recall  f1-score   support
    
         class 0       1.00      1.00      1.00         2
         class 1       0.50      0.50      0.50         2
         class 2       0.67      0.67      0.67         3
    
        accuracy                           0.71         7
       macro avg       0.72      0.72      0.72         7
    weighted avg       0.71      0.71      0.71         7
    


##### ROC ì»¤ë¸ŒÂ¶
    ìœ„ì—ì„œ ì„¤ëª…í•œ ê°ì¢… í‰ê°€ ì ìˆ˜ ì¤‘ ì¬í˜„ìœ¨(recall)ê³¼ ìœ„ì–‘ì„±ë¥ (fall-out)ì€ ì¼ë°˜ì ìœ¼ë¡œ ì–‘ì˜ ìƒê´€ ê´€ê³„ê°€ ìˆë‹¤.
    ROC(Receiver Operator Characteristic) ì»¤ë¸ŒëŠ” í´ë˜ìŠ¤ íŒë³„ ê¸°ì¤€ê°’ì˜ ë³€í™”ì— ë”°ë¥¸ ìœ„ì–‘ì„±ë¥ (fall-out)ê³¼ ì¬í˜„ìœ¨(recall)ì˜ ë³€í™”ë¥¼ ì‹œê°í™”


```python
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression

X, y = make_classification(n_samples=16, n_features=2,
                           n_informative=2, n_redundant=0,
                           random_state=0)

model = LogisticRegression().fit(X, y)
y_hat = model.predict(X)
f_value = model.decision_function(X)

df = pd.DataFrame(np.vstack([f_value, y_hat, y]).T, columns=["f", "y_hat", "y"])
df.sort_values("f", ascending=False).reset_index(drop=True)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>f</th>
      <th>y_hat</th>
      <th>y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2.363163</td>
      <td>1.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2.065047</td>
      <td>1.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.633657</td>
      <td>1.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1.626171</td>
      <td>1.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.624967</td>
      <td>1.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>1.219678</td>
      <td>1.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.378296</td>
      <td>1.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.094285</td>
      <td>1.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>-0.438666</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>-0.765888</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>-0.926932</td>
      <td>0.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>-1.046770</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>-1.214700</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>-1.429382</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>-2.081586</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>-4.118969</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>




```python
confusion_matrix(y, y_hat, labels=[1, 0])
```




    array([[7, 1],
           [1, 7]])




```python
recall = 7 / (6 + 2)
fallout = 1 / (1 + 7)
print("recall =", recall)
print("fallout =", fallout)
```

    recall = 0.875
    fallout = 0.125



```python
from sklearn.metrics import roc_curve

fpr, tpr, thresholds = roc_curve(y, model.decision_function(X))
fpr, tpr, thresholds
```




    (array([0.   , 0.   , 0.   , 0.125, 0.125, 0.375, 0.375, 1.   ]),
     array([0.   , 0.125, 0.75 , 0.75 , 0.875, 0.875, 1.   , 1.   ]),
     array([ 3.36316277,  2.36316277,  1.21967832,  0.37829565,  0.09428499,
            -0.76588836, -0.92693183, -4.11896895]))




```python
fpr, tpr, thresholds = roc_curve(y, model.predict_proba(X)[:, 1])
fpr, tpr, thresholds
```




    (array([0.   , 0.   , 0.   , 0.125, 0.125, 0.375, 0.375, 1.   ]),
     array([0.   , 0.125, 0.75 , 0.75 , 0.875, 0.875, 1.   , 1.   ]),
     array([1.9139748 , 0.9139748 , 0.77200693, 0.59346197, 0.5235538 ,
            0.31736921, 0.28354759, 0.01600107]))




```python
plt.plot(fpr, tpr, 'o-', label="Logistic Regression")
plt.plot([0, 1], [0, 1], 'k--', label="random guess")
plt.plot([fallout], [recall], 'ro', ms=10)
plt.xlabel('ìœ„ì–‘ì„±ë¥ (Fall-Out)')
plt.ylabel('ì¬í˜„ë¥ (Recall)')
plt.title('Receiver operating characteristic example')
plt.show()
```


<img width="393" alt="output_43_0" src="https://user-images.githubusercontent.com/59719711/82555629-1014e200-9ba3-11ea-9fcf-25fe1c5c9d38.png">



```python

```


```python

```


```python

```


```python

```


```python

```


```python

```


```python

```


```python

```
