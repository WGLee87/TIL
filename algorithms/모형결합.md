```python
import matplotlib
from matplotlib import font_manager, rc
import platform

try :
    if platform.system() == 'windows':
        # windows의 경우
        font_name = font_manager.FomntProperties(fname="c:/Windows/Font")
        rc('font', family = font_name)
    else:
        # mac의 경우
        rc('font', family = 'AppleGothic')
except :
    pass

matplotlib.rcParams['axes.unicode_minus'] = False
```


```python
X = np.array([[0, -0.5], [-1.5, -1.5], [1, 0.5], [-3.5, -2.5], [0, 1], [1, 1.5], [-2, -0.5]])
y = np.array([1, 1, 1, 2, 2, 2, 2])
x_new = [0, -1.5]

plt.scatter(X[y == 1, 0], X[y == 1, 1], s=100, marker='o', c='r', label="클래스 1")
plt.scatter(X[y == 2, 0], X[y == 2, 1], s=100, marker='x', c='b', label="클래스 2")
plt.scatter(x_new[0], x_new[1], s=100, marker='^', c='g', label="테스트 데이터")
plt.xlabel("x1")
plt.ylabel("x2")
plt.title("이진 분류 예제 데이터")
plt.legend()

plt.show()
```


<img width="387" alt="output_1_0" src="https://user-images.githubusercontent.com/59719711/83350557-cfbe1c80-a377-11ea-9670-5acb83803927.png">




```python
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.ensemble import VotingClassifier

model1 = LogisticRegression(random_state=1)
model2 = QuadraticDiscriminantAnalysis()
model3 = GaussianNB()

ensemble = VotingClassifier(estimators=[('lr', model1), ('qda', model2), ('gnb', model3)], voting='soft')

probas = [c.fit(X, y).predict_proba([x_new]) for c in (model1, model2, model3, ensemble)]
class1_1 = [pr[0, 0] for pr in probas]
class2_1 = [pr[0, 1] for pr in probas]

ind = np.arange(4)
width = 0.35  # bar width
p1 = plt.bar(ind, np.hstack(([class1_1[:-1], [0]])), width, color='green')
p2 = plt.bar(ind + width, np.hstack(([class2_1[:-1], [0]])), width, color='lightgreen')
p3 = plt.bar(ind, [0, 0, 0, class1_1[-1]], width, color='blue')
p4 = plt.bar(ind + width, [0, 0, 0, class2_1[-1]], width, color='steelblue')

plt.xticks(ind + 0.5 * width, ['로지스틱 회귀 모형', 'QDA 모형', '가우시안 나이브베이즈', '소프트 다수결 모형'])
plt.ylim([0, 1.1])
plt.title('세가지 다른 분류 모형과 소프트 다수결 모형의 분류 결과')
plt.legend([p1[0], p2[0]], ['클래스 1', '클래스 2'], loc='upper left')
plt.show()
```


<img width="381" alt="output_2_0" src="https://user-images.githubusercontent.com/59719711/83350548-c59c1e00-a377-11ea-8f62-b3637bf18eb0.png">




```python
from itertools import product

x_min, x_max = -4, 2
y_min, y_max = -3, 2
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.005),
                     np.arange(y_min, y_max, 0.005))
f, axarr = plt.subplots(2, 2)
for idx, clf, tt in zip(product([0, 1], [0, 1]),
                        [model1, model2, model3, ensemble],
                        ['로지스틱 회귀', 'QDA', '가우시안 나이브베이즈', '다수결 모형']):
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    axarr[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.2, cmap=mpl.cm.jet)
    axarr[idx[0], idx[1]].scatter(
        X[:, 0], X[:, 1], c=y, alpha=0.5, s=50, cmap=mpl.cm.jet)
    axarr[idx[0], idx[1]].scatter(x_new[0], x_new[1], marker='x')
    axarr[idx[0], idx[1]].set_title(tt)
plt.tight_layout()
plt.show()
```


<img width="420" alt="output_3_0" src="https://user-images.githubusercontent.com/59719711/83350568-e1072900-a377-11ea-8ac9-f19adfd0b29e.png">


###### 연습문제
    iris 데이터를 가지고 모형 결합을 통해 교차검증 퍼포먼스 점수 출력하기


```python
from sklearn.datasets import load_iris
iris = load_iris()
# df = pd.DataFrame(iris.data, columns=iris.feature_names)
# y = pd.Series(iris.target, dtype='category')
# y = y.cat.rename_categories(iris.target_names)
# df['species'] = y
# df
```


```python
iris = load_iris()
idx = np.in1d(iris.target, [0,1])
x = iris.data[idx, :2]
x0 = sm.add_constant(x)
y = iris.target[idx]
```


```python
from sklearn.linear_model import LogisticRegression
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import BernoulliNB
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier

from sklearn.ensemble import VotingClassifier

model1 = LogisticRegression(random_state=1)
model2 = QuadraticDiscriminantAnalysis()
model3 = LinearDiscriminantAnalysis()
model4 = GaussianNB()
model5 = BernoulliNB()
model6 = MultinomialNB()
model7 = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0)

ensemble = VotingClassifier(estimators=[('lr', model1), ('qda', model2), ('lda', model3), ('gnb', model4), ('bern', model5), \
                           ('mnb', model6), ('tree', model7)], voting='soft', weights=[1,1,1,1,1,1,2])

ens_model = ensemble.fit(x,y)

# # 예측된 y값
# y_pred = ens_model.predict(x)

from sklearn.model_selection import cross_val_score, KFold
cv = KFold(5, shuffle=True, random_state=0)
cross_val_score(ens_model, x, y, scoring='accuracy', cv=cv).mean()
```




    0.9800000000000001




```python
y_pred, y
```




    (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
            0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
            1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
            1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),
     array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
            1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
            1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]))




```python

```

###### 연습 문제 2
    breast cancer 분류 문제를 Bagging을 사용하여 풀어라. 모형의 종류 및 개수나 Bagging 방법은 마음대로 한다. K=5인 교차 검증을 하였을 때 성능의 평균과 표준편차를 구하라.
    bagging 모형의 성능을 개별 모형과 비교하라.


```python
from sklearn.datasets import load_breast_cancer
bc = load_breast_cancer()
bc_x = bc.data
bc_y = bc.target

from sklearn.ensemble import BaggingClassifier
model1 = DecisionTreeClassifier(max_depth=10, random_state=0).fit(bc_x, bc_y)
model2 = BaggingClassifier(DecisionTreeClassifier(max_depth=2), n_estimators=100, random_state=0).fit(bc_x, bc_y)

cv = KFold(5, shuffle=True, random_state=0)
model1_cross = cross_val_score(model1, bc_x, bc_y, scoring='accuracy', cv=cv)
model2_cross = cross_val_score(model2, bc_x, bc_y, scoring='accuracy', cv=cv)
print('개별모형의 성능점수(평균) : {} '.format(model1_cross.mean()))
print('개별모형의 성능점수(표준편차) : {} '.format(model1_cross.std()))
print('Bagging모형의 성능점수(평균) : {}'.format(model2_cross.mean()))
print('Bagging모형의 성능점수(표준편차) : {}'.format(model2_cross.std()))
```

    개별모형의 성능점수(평균) : 0.9244993013507219 
    개별모형의 성능점수(표준편차) : 0.021797968056032534 
    Bagging모형의 성능점수(평균) : 0.9437820214252446
    Bagging모형의 성능점수(표준편차) : 0.021153876610708496


###### 연습 문제
    breast cancer 분류 문제를 Extreme 랜덤포레스트를 사용하여 풀어라. K=5인 교차 검증을 하였을 때 평균 성능을 구하라.
    특징 중요도를 구하라. 어떤 특징들이 판별에 중요하게 사용되는가?


```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import ExtraTreesClassifier
model_rf = RandomForestClassifier(max_depth=10, n_estimators=100, random_state=0).fit(bc_x, bc_y)
model_rf0 = ExtraTreesClassifier(n_estimators=100, random_state=0).fit(bc_x, bc_y)
cv = KFold(5, shuffle=True, random_state=0)
print(cross_val_score(model_rf, bc_x, bc_y, scoring='accuracy', cv=cv).mean())
print(cross_val_score(model_rf0, bc_x, bc_y, scoring='accuracy', cv=cv).mean())
```

    0.9613724576929048
    0.9736842105263157



```python
# 특성 중요도
idx = np.argsort(model_rf.feature_importances_)
names = bc.feature_names[idx]
values = model_rf.feature_importances_[idx]

plt.figure(figsize=(12,6))
plt.barh(names, values)
plt.title('Breast Cancer Feature Importance')
plt.show()
```


<img width="810" alt="output_14_0" src="https://user-images.githubusercontent.com/59719711/83350578-f714e980-a377-11ea-8ecb-01d2283286a5.png">



```python

```
